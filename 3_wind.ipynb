{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "V_XYn7oZOzWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1GSAF_u7Gxz0Wcxjd-7zL1Gd2WfC0s7Zz\n",
        "!unzip wind-turbine-damage-challenges.zip\n",
        "!rm wind-turbine-damage-challenges.zip"
      ],
      "metadata": {
        "id": "nx2eCf6HO5TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "from ultralytics import YOLO\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "mf0cI7alPClM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b72a86-73e4-4de4-bd09-c8f6f04a88ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpHy1iY5Ogl6"
      },
      "outputs": [],
      "source": [
        "def load_coco_annotations(annotation_path):\n",
        "    with open(annotation_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def convert_coco_to_yolo(coco_bbox, img_width, img_height):\n",
        "    x, y, w, h = coco_bbox\n",
        "    x_center = x + w/2\n",
        "    y_center = y + h/2\n",
        "\n",
        "    # Нормализация координат для YOLO формата\n",
        "    x_center = x_center / img_width\n",
        "    y_center = y_center / img_height\n",
        "    w = w / img_width\n",
        "    h = h / img_height\n",
        "\n",
        "    return [x_center, y_center, w, h]\n",
        "\n",
        "def process_image(image_path, annotation_data, type_folder, output_size=640, overlap=0.5):\n",
        "    img = cv2.imread(image_path)\n",
        "    img_height, img_width = img.shape[:2]\n",
        "\n",
        "    image_id = next(img_info['id'] for img_info in annotation_data['images']\n",
        "                   if img_info['file_name'] == os.path.basename(image_path))\n",
        "    annotations = [ann for ann in annotation_data['annotations'] if ann['image_id'] == image_id]\n",
        "\n",
        "    output_img_dir = Path(f'processed_dataset/images/{type_folder}')\n",
        "    output_label_dir = Path(f'processed_dataset/labels/{type_folder}')\n",
        "    output_img_dir.mkdir(parents=True, exist_ok=True)\n",
        "    output_label_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    step = int(output_size * (1 - overlap))\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    # Реализация скользящего окна\n",
        "    for y in range(0, img_height - output_size + 1, step):\n",
        "        for x in range(0, img_width - output_size + 1, step):\n",
        "            crop = img[y:y+output_size, x:x+output_size]\n",
        "\n",
        "            yolo_annotations = []\n",
        "\n",
        "            for ann in annotations:\n",
        "                bbox = ann['bbox']\n",
        "                category_id = ann['category_id']-1\n",
        "\n",
        "                # Проверяем, находится ли bbox в текущем окне\n",
        "                if (x <= bbox[0] <= x + output_size and\n",
        "                    y <= bbox[1] <= y + output_size):\n",
        "\n",
        "                    # Корректируем координаты bbox относительно окна\n",
        "                    # обрезаем если выходит за границы\n",
        "                    new_bbox = [\n",
        "                        bbox[0] - x,\n",
        "                        bbox[1] - y,\n",
        "                        min(bbox[2], output_size - (bbox[0] - x)),\n",
        "                        min(bbox[3], output_size - (bbox[1] - y))\n",
        "                    ]\n",
        "\n",
        "                    yolo_bbox = convert_coco_to_yolo(new_bbox, output_size, output_size)\n",
        "\n",
        "                    # Проверяем, что все координаты в допустимом диапазоне [0, 1]\n",
        "                    if all(0 <= coord <= 1 for coord in yolo_bbox):\n",
        "                        yolo_annotations.append(f\"{category_id} {' '.join(map(str, yolo_bbox))}\")\n",
        "\n",
        "            if yolo_annotations:\n",
        "                output_img_path = output_img_dir / f\"{Path(image_path).stem}_{counter}.jpg\"\n",
        "                cv2.imwrite(str(output_img_path), crop)\n",
        "\n",
        "                output_label_path = output_label_dir / f\"{Path(image_path).stem}_{counter}.txt\"\n",
        "                with open(output_label_path, 'w') as f:\n",
        "                    f.write('\\n'.join(yolo_annotations))\n",
        "\n",
        "                counter += 1\n",
        "\n",
        "def process_dataset(input_dir, annotation_path, type_folder):\n",
        "    annotation_data = load_coco_annotations(annotation_path)\n",
        "\n",
        "    for img_path in tqdm(list(Path(input_dir).glob('*.jpg'))):\n",
        "        process_image(str(img_path), annotation_data, type_folder)\n",
        "\n",
        "\n",
        "def train_yolov8():\n",
        "    # Создаем файл конфигурации датасета\n",
        "    dataset_yaml = \"\"\"\n",
        "    path: /content/processed_dataset/\n",
        "    train: images/train\n",
        "    val: images/valid\n",
        "    names:\n",
        "      0: corrosion\n",
        "      1: lightning\n",
        "      2: lightning receptor\n",
        "      3: missing teeth\n",
        "      4: patch\n",
        "    \"\"\"\n",
        "\n",
        "    with open('dataset.yaml', 'w') as f:\n",
        "        f.write(dataset_yaml)\n",
        "\n",
        "    model = YOLO('yolov8n.pt')\n",
        "\n",
        "    model.train(data='dataset.yaml', epochs=200, imgsz=640, batch=16, save_period=5, exist_ok=True,project=\"detect_turbines\", name=\"yolo8-detect\", resume=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Один раз только запускать\n",
        "process_dataset(\"Wind Turbine damage/train\", \"Wind Turbine damage/train/_annotations.coco.json\", \"train\")\n",
        "process_dataset(\"Wind Turbine damage/valid\", \"Wind Turbine damage/valid/_annotations.coco.json\", \"valid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqMFcw1ePJlf",
        "outputId": "66135e5b-0ce7-4d25-ce28-172eabbf2bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1070/1070 [01:29<00:00, 11.99it/s]\n",
            "100%|██████████| 271/271 [00:16<00:00, 16.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_yolov8()"
      ],
      "metadata": {
        "id": "tM-xQ9YBPLnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submit"
      ],
      "metadata": {
        "id": "ye22rUPqMB8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1ePuyQSNw6FrvfsOcL0h2Z79d-VD2bgwf\n",
        "!unzip sample_submission_with_errors.csv.zip\n",
        "!rm sample_submission_with_errors.csv.zip"
      ],
      "metadata": {
        "id": "cUnL-g2eMDrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image_for_inference(image_path, model, output_size=640, overlap=0.5):\n",
        "    img = cv2.imread(image_path)\n",
        "    img_height, img_width = img.shape[:2]\n",
        "\n",
        "    temp_dir = Path('temp_inference')\n",
        "    temp_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    step = int(output_size * (1 - overlap))\n",
        "\n",
        "    class_names = [\n",
        "    \"corrosion\",\n",
        "    \"lightning\",\n",
        "    \"lightning receptor\",\n",
        "    \"missing teeth\",\n",
        "    \"patch\"\n",
        "    ]\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    for y in range(0, img_height - output_size + 1, step):\n",
        "        for x in range(0, img_width - output_size + 1, step):\n",
        "            crop = img[y:y+output_size, x:x+output_size]\n",
        "\n",
        "            temp_img_path = temp_dir / f\"{Path(image_path).stem}_temp.jpg\"\n",
        "            cv2.imwrite(str(temp_img_path), crop)\n",
        "\n",
        "            results = model(temp_img_path, conf=0.25, verbose=False)\n",
        "\n",
        "            for result in results:\n",
        "                boxes = result.boxes\n",
        "\n",
        "                for box in boxes:\n",
        "                    class_index = int(box.cls[0].cpu().numpy())\n",
        "                    class_name = class_names[class_index]\n",
        "                    conf = float(box.conf[0].cpu().numpy())\n",
        "\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "\n",
        "                    x1 = float(x1 + x)\n",
        "                    y1 = float(y1 + y)\n",
        "                    x2 = float(x2 + x)\n",
        "                    y2 = float(y2 + y)\n",
        "\n",
        "                    bbox = [x1, y1, x2 - x1, y2 - y1]\n",
        "\n",
        "                    predictions.append([f'\"{class_name}\"'] + bbox + [conf])\n",
        "\n",
        "    predictions = non_max_suppression(predictions, iou_threshold=0.3)\n",
        "\n",
        "    shutil.rmtree(temp_dir)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def non_max_suppression(predictions, iou_threshold):\n",
        "    \"\"\"\n",
        "    Применяет NMS к предсказаниям для удаления дублирующихся боксов\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "    # Группируем предсказания по классам для более точной фильтрации\n",
        "    class_predictions = {}\n",
        "\n",
        "    for pred in predictions:\n",
        "        class_name = pred[0]\n",
        "        if class_name not in class_predictions:\n",
        "            class_predictions[class_name] = []\n",
        "        class_predictions[class_name].append(pred)\n",
        "\n",
        "    for class_name, preds in class_predictions.items():\n",
        "        preds = sorted(preds, key=lambda x: x[5], reverse=True)\n",
        "\n",
        "        while preds:\n",
        "            current = preds.pop(0)\n",
        "            filtered.append(current)\n",
        "\n",
        "            adaptive_iou = iou_threshold * (1 + current[5]) / 2\n",
        "            preds = [pred for pred in preds if not check_overlap(current[1:5], pred[1:5], adaptive_iou)]\n",
        "\n",
        "    return filtered\n",
        "\n",
        "def check_overlap(box1, box2, threshold):\n",
        "    \"\"\"\n",
        "    Проверяет перекрытие между двумя боксами\n",
        "    \"\"\"\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[0] + box1[2], box2[0] + box2[2])\n",
        "    y2 = min(box1[1] + box1[3], box2[1] + box2[3])\n",
        "\n",
        "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "    area1 = box1[2] * box1[3]\n",
        "    area2 = box2[2] * box2[3]\n",
        "\n",
        "    union = area1 + area2 - intersection\n",
        "    if union <= 0:\n",
        "        return False\n",
        "\n",
        "    iou = intersection / union\n",
        "    return iou > threshold\n",
        "\n",
        "def create_submission_file(predictions, output_file='submission.csv'):\n",
        "    df = pd.DataFrame(predictions)\n",
        "    df.to_csv(output_file, index=False, sep=',')\n",
        "\n",
        "def main():\n",
        "    model = YOLO('/content/best_large.pt')\n",
        "\n",
        "    df_example = pd.read_csv(\"/content/sample_submission_with_errors.csv\")\n",
        "    images = df_example[\"image_id\"].to_list()\n",
        "\n",
        "    results = []\n",
        "    for image_id in tqdm(images):\n",
        "        img_path = os.path.join(\"Wind Turbine damage\", \"test\", image_id)\n",
        "        predictions = process_image_for_inference(str(img_path), model)\n",
        "        results.append({\"image_id\": str(image_id), \"objects\": str(predictions)})\n",
        "\n",
        "    create_submission_file(results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "kPumBukb0laI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e204f6fa-a824-423c-d480-5c7677d73e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 151/151 [24:43<00:00,  9.82s/it]\n"
          ]
        }
      ]
    }
  ]
}